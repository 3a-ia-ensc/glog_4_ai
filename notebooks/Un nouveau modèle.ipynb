{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path\n",
    "import string\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from src.Dataset import Dataset\n",
    "from src.visualisations import *\n",
    "from src.data_processing import *\n",
    "from src.model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 7100 rows\n"
     ]
    }
   ],
   "source": [
    "train_set = path.join('..', 'data', 'training_set.json')\n",
    "test_set = path.join('..', 'data', 'testing_set.json')\n",
    "data = Dataset(train_set, test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Création d'un nouveau modèle\n",
    "## Traitement des données\n",
    "### Traitement du déséquilibre interclasses\n",
    "Avant toute chose, il est nécessaire de résoudre le problème du déséquilibre entre le nombre de phrases dans chaque classe.\n",
    "Pour cela, nous avons décidé de simplement retirer un certain nombre de phrases (aléatoirement) de la classe \"irrelevant\" afin que toutes les classes aient en moyenne à peu près le même nombre de phrases que les autres classes\n",
    "\n",
    "Peut-être :\n",
    "- Tout mettre au meme niveau\n",
    "- Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'reset_index'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-d01b46c69de2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbalanced_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbalance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'reset_index'"
     ]
    }
   ],
   "source": [
    "balanced_data = balance(data).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Préparation pour l'entrainement\n",
    "#### Vectorisation\n",
    "On vectorise tout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_sentences = vectorize_data(balanced_data['sentence'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One hot encodage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, one_hot_labels = to_one_hot(balanced_data['intent'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Le modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NlpModel(vectorized_sentences.shape[1])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On redécoupe les données en en gardant 20% pour tester le modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_train, sentences_test, y_train, y_test = cut_data(vectorized_sentences, one_hot_labels, 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On entraine le modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = model.train(sentences_train, y_train, sentences_test, y_test, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluations\n",
    "Evaluons le modèle pour le comparer avec le modèle que nous avions précédemment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = model._model.evaluate(sentences_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predic_prob = model.predict(sentences_test)\n",
    "predictions = np.argmax(y_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_analysis(predictions, np.argmax(y_test, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"Compute and display roc curve for each class\n",
    "\n",
    "Parameters:\n",
    "predictions_prob (array): Table of predictions (must contain probability for each class)\n",
    "\n",
    "\"\"\"\n",
    "classes = ['find-train', 'irrelevant', 'find-flight', 'find-restaurant',\n",
    "           'purchase', 'find-around-me', 'provide-showtimes', 'find-hotel']\n",
    "\n",
    "\n",
    "\n",
    "fpr = {}\n",
    "tpr = {}\n",
    "auc = {}\n",
    "thresh = {}\n",
    "\n",
    "for cl in range(8):\n",
    "    fpr[cl], tpr[cl], thresh[cl] = roc_curve(np.argmax(y_test, axis=1)==cl, predic_prob[:, cl])\n",
    "\n",
    "    y_pred = predictions == cl\n",
    "    y_true = np.argmax(y_test, axis=1) == cl\n",
    "    auc[cl] = roc_auc_score(y_true, y_pred)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_shape(\n",
    "    type='line', line=dict(dash='dash'),\n",
    "    x0=0, x1=1, y0=0, y1=1\n",
    ")\n",
    "\n",
    "for i in range(len(classes)):\n",
    "    cl = i\n",
    "    name = f\"{cl} (AUC={auc[cl]:.2f})\"\n",
    "    fig.add_trace(go.Scatter(x=fpr[cl], y=tpr[cl], name=name, mode='lines'))\n",
    "\n",
    "fig.update_layout(\n",
    "    xaxis_title='False Positive Rate',\n",
    "    yaxis_title='True Positive Rate',\n",
    "    yaxis=dict(scaleanchor=\"x\", scaleratio=1),\n",
    "    xaxis=dict(constrain='domain'),\n",
    "    width=700, height=500\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sauvegarde\n",
    "Pour finir, on sauvegarde le modèle afin de l'utiliser dans une API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.models.save_model(model._model, filepath='../models/model_11_12_2020.hdf5', save_format='h5')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import nltk\n",
    "nltk.download('omw')\n",
    "\n",
    "syn = [synset.lemma_names('fra') for synset in wordnet.synsets('programme', lang='fra')]\n",
    "print(syn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
